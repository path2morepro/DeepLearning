{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboration\n",
    "\n",
    "### **Quick introduction to Jupiter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Rerunning it deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "* A useful debug tool is the console. You can right-click anywhere in the notebook and select \"New console for notebook\". This opens a python console which shares the environment with the notebook, which let's you easily print variables or test commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setups\n",
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Your task**\n",
    "Your task is to make a DNN that can classify benign or malicious networks attacks using the Mirai dataset (see below). \n",
    "\n",
    "**You need to answer all the questions in the notebook.** Also, for all classification tasks that you will explore, you should always answer these two questions:\n",
    "- How good classification accuracy can a naive classifier obtain? The naive classifier will assume that all examples belong to one class.\n",
    "- What is random chance classification accuracy if you randomly guess the label of each (test) example? For a balanced dataset and binary classification this is easy (50%), but in many cases it is more complicated and a Monte Carlo simulation may be required to estimate random chance accuracy.\n",
    "\n",
    "If your classifier cannot perform better than a naive classifier or a random classifier, you are doing something wrong.\n",
    "\n",
    "If the training is too slow on your own computer, use the smaller datasets (*half or *quarter).\n",
    "\n",
    "Dense networks are not optimal for tabular datasets like the one used here, but here the main goal is to explore and get a a hands-on experience with deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Dataset #\n",
    "\n",
    "Data used in this laboration are from the [Kitsune Network Attack Datase](https://archive.ics.uci.edu/ml/datasets/Kitsune+Network+Attack+Dataset). We will focus on the 'Mirai' part of the dataset. Your task is to make a DNN that can classify if each attack is benign or malicious. The dataset has 116 covariates, but to make it a bit more difficult we will remove the first 24 covariates.\n",
    "\n",
    "### **1.1 Load the data**\n",
    "Complete and run the following cell to to load the the `Mirai_data.npy` and the `Mirai_labels.npy` files and remove the first 24 covariances to make the classification task harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt # ! Not needed if you load data from numpy arrays !\n",
    "import numpy as np\n",
    "\n",
    "# Load data from numpy arrays, choose reduced files if the training takes too long\n",
    "# Load the dataset\n",
    "X = np.load('Mirai_data.npy')\n",
    "Y = np.load('Mirai_labels.npy')\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Remove the first 24 covariates (columns)\n",
    "X = ???\n",
    "Y = ???\n",
    "\n",
    "# Print the size of the covariates and labels\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Explore the data (NaNs)**\n",
    "It is common to have NaNs (not a number) in the data, lets check for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# It is common to have NaNs in the data, lets check for it. Hint: np.isnan()\n",
    "???\n",
    "# Fist check for NaNs in the data and then in the labels\n",
    "???\n",
    "# Print the number of NaNs in the covariates\n",
    "???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Data preprocessing: normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Convert covariates to floats\n",
    "X = ???\n",
    "\n",
    "# Convert labels to integers\n",
    "Y = ???\n",
    "\n",
    "# Remove mean of each covariate (column)\n",
    "X = ???\n",
    "\n",
    "# Divide each covariate (column) by its standard deviation\n",
    "X = ???\n",
    "\n",
    "# Check that mean is 0 and standard deviation is 1 for all covariates, by printing mean and std\n",
    "???\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 Data split**\n",
    "Use the first 70% of the dataset for training, leave the other 30% for validation and test, call the variables:\n",
    "- `Xtrain` and `Ytrain`  (70% of the dataset)\n",
    "- `Xtemp` and `Ytemp`  (30% of the dataset)\n",
    "\n",
    "We use a function from scikit learn (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# split the original dataset into 70% Training and 30% Temp\n",
    "Xtrain, Xtemp, Ytrain, Ytemp = ???\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Xtrain has size {}.'.format(Xtrain.shape))\n",
    "print('Ytrain has size {}.'.format(Ytrain.shape))\n",
    "\n",
    "print('Xtemp has size {}.'.format(Xtemp.shape))\n",
    "print('Ytemp has size {}.'.format(Ytemp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split your non-training data (`Xtemp`, `Ytemp`) into 50% validation (`Xval`, `Yval`) and 50% testing (`Xtest`, `Ytest`), we use a function from scikit learn. In total this gives us 70% for training, 15% for validation, 15% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# split the remaining 30% into 50% Validation and 50% Test\n",
    "Xval, Xtest, Yval, Ytest = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f'The validation set has size {Xval.shape[0]}')\n",
    "print(f'The test set has size {Xtest.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "1. Do all variables (`Xtrain`,`Ytrain`), (`Xval`,`Yval`), (`Xtest`,`Ytest`) have the shape that you expect?\n",
    "2. Given the number of examples from each class, how high classification performance can a naive classifier obtain? The naive classifier will assume that all examples belong to one class. Note: you do not need to make a naive classifier, this is a theoretical question, just to understand how good performance we can obtain by guessing that all examples belong to one class.\n",
    "\n",
    "Note, that if your classifier cannot perform better than a naive classifier or a random classifier, you are doing something wrong.\n",
    "\n",
    "\n",
    "#### **<span style=\"color:green\">Answer</span>**\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# This sets the GPU to allocate memory only as needed\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) != 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "else:\n",
    "    print('No GPU available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: DNN classification\n",
    "In this next section you will define utilities for building the deep learning networks that will be used later and for visualizing the model training. You will also train several model experimenting with different model architecture configurations and methods for model regularization.\n",
    "\n",
    "### **2.1 Build DNN model**\n",
    "Implement the `build_DNN` and `plot_results` functions in the `utilities.py` file. Note that for the changes in the `utilities.py` definitions to be visible by the notebook, you need to save the file. \n",
    "\n",
    "Here are some relevant functions that you should use in `build_DNN`. For a complete list of functions and their definitions see the [keras documentation](https://keras.io/api/):\n",
    "\n",
    "- `model.add()`, adds a layer to the network;\n",
    "- `Dense()`, a dense network layer. See the [documentation](https://keras.io/api/layers/core_layers/dense/) what are the input options and outputs of the `Dense()` function. \n",
    "- `model.compile()`, compiles the model. You can set the input metrics=['accuracy'] to print the classification accuracy during the training.\n",
    "- cost and loss functions: check the [documentation](https://keras.io/losses/) and chose a loss function for binary classification.\n",
    "\n",
    "To get more information in model [compile](https://keras.io/api/models/model_training_apis/#compile-method), [training](https://keras.io/api/models/model_training_apis/#fit-method) and [evaluation](https://keras.io/api/models/model_training_apis/#evaluate-method) see the relevant documentation.\n",
    "\n",
    "After defining the`build_DNN` function use it to create the your first DNN classifier. Start with a simple network with 2 dense layers (with 20 nodes each), using sigmoid activation functions. The final dense layer should have a single node and a sigmoid activation function. We start with the SGD optimizer.\n",
    "\n",
    "Make sure that the last layer always has a sigmoid activation function (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import build_DNN, plot_results\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# import a suitable loss function from keras.losses and use as input to the build_DNN function.\n",
    "from tf_keras.losses import ???\n",
    "\n",
    "# Build a DNN model following the specifications above\n",
    "model = build_DNN(???, ???, ???, ???, ???)\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Train DNN model**\n",
    "\n",
    "Time to train the DNN!\n",
    "Start simple with 2 hidden layers with 20 nodes each.\n",
    "\n",
    "Build set the different hyper-parameters, build the model and run the training. Use the following training and hyper-parameters:\n",
    "- `batch_size=20`\n",
    "- `epochs=20`\n",
    "- `learning_rate=0.1`\n",
    "\n",
    "Make sure that you are using learning rate 0.1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hidden layers with 20 nodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training and hyper-parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Specify the learning rate, the input shape and the loss function\n",
    "learning_rate = ???\n",
    "input_shape = ???\n",
    "loss = ???\n",
    "\n",
    "# Build the model\n",
    "model1 = ???\n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "history1 = ???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Evaluate the model on the test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_results\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "\n",
    "3. What happens if you add several Dense layers without specifying the activation function?\n",
    "\n",
    "4. How are the weights in each dense layer initialized as default? How are the bias weights initialized?\n",
    "\n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Adressing class imbalance**\n",
    "\n",
    "This dataset is rather unbalanced with the majority of the samples belonging to class=1. We need to define class weights so that the training pays more attention to the class with fewer samples. We use the [`compute_class_weight`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) function from `scikit-learn`.\n",
    "\n",
    "You need to call the function something like this\n",
    "```python\n",
    "class_weights = class_weight.compute_class_weight(class_weight = , classes = , y = )\n",
    "```\n",
    "\n",
    "otherwise it will through an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Calculate class weights\n",
    "value1, value2 = ???\n",
    "\n",
    "# Print the class weights\n",
    "???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Convert class weights into a dictionary that can be used as input to the model.fit() function\n",
    "\n",
    "class_weights = {0: value1,\n",
    "                1: value2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model using class weights. 2 hidden layers with 20 nodes each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training and hyper-parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Specify the learning rate, the input shape and the loss function\n",
    "learning_rate = ???\n",
    "input_shape = ???\n",
    "loss = ???\n",
    "\n",
    "# Build the model\n",
    "model2 = ???\n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "history2 = ???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "# Evaluate model on test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additional questions**\n",
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "\n",
    "5. Why do we have to use a batch size? Why can't we simply use all data at once? This is more relevant for even larger datasets.\n",
    "\n",
    "6. What is the processing time for one training epoch when the batch size is 100? What is the processing time for one epoch when the batch size is 1,000? What is the processing time for one epoch when the batch size is 10,000? Explain the results. \n",
    "\n",
    "7. How many times are the weights in the DNN updated in each training epoch if the batch size is 100? How many times are the weights in the DNN updated in each training epoch if the batch size is 1,000? How many times are the weights in the DNN updated in each training epoch if the batch size is 10,000?  \n",
    "\n",
    "8. What limits how large the batch size can be?\n",
    "\n",
    "9.  Generally speaking, how is the learning rate related to the batch size? If the batch size is decreased, how should the learning rate be changed?\n",
    "10. How many trainable parameters does the network with 4 dense layers with 50 nodes each have, compared to the initial network with 2 layers and 20 nodes per layer? Hint: use model.summary()\n",
    "   \n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Model regularization**\n",
    "\n",
    "In the following sections you will explore methods for model normalization, namely `BatchNormalization` and `Dropout`, and also look at the impact of ofter activation functions and optimization algorithms.\n",
    "#### **2.3.1 Batch normalization**\n",
    "\n",
    "Now add batch normalization after each hidden dense layer in `build_DNN`.\n",
    "\n",
    "See the [documentation](https://keras.io/layers/normalization/) for information about how to call the function.\n",
    "\n",
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "11. Why is batch normalization important when training deep networks?\n",
    "\n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hidden layers, 20 nodes each, class weights and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Build and train model\n",
    "model6 = ???\n",
    "\n",
    "history6 = ???\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.2 Activation function**\n",
    "\n",
    "Try changing the activation function in each layer from sigmoid to [ReLU](https://keras.io/api/layers/activations/).\n",
    "\n",
    "**Note**: the last layer should still have a sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hidden layers, 20 nodes each, class weights, ReLU and no batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Build and train model\n",
    "model7 = ???\n",
    "\n",
    "history7 = ???\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.3 Optimizer**\n",
    "\n",
    "Try changing the optimizer from SGD to Adam (with learning rate 0.1 as before). Remember to import the Adam optimizer from [keras.optimizers](https://keras.io/optimizers/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hidden layers, 20 nodes each, class weights, Adam optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Build and train model\n",
    "model8 = ???\n",
    "\n",
    "history8 = ???\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 **Dropout regularization**\n",
    "\n",
    "Dropout is a type of regularization that can improve accuracy for validation and test data. It randomly removes connections to force the neural network to not rely too much on a small number of weights.\n",
    "\n",
    "Add a Dropout layer after each Dense layer (but not after the final dense layer) in `build_DNN`, with a dropout probability of 50%. Look at the [documentation](https://keras.io/api/layers/regularization_layers/dropout/) for more information on how to call set this layer.\n",
    "\n",
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "12. How does the validation accuracy change when adding dropout?\n",
    "13. How does the test accuracy change when adding dropout?\n",
    "\n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hidden layers with 20 nodes each, class weights, dropout, SGD optimizer, no batch normalization and sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Build and train model\n",
    "model9 = ???\n",
    "\n",
    "history9 = ???\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Hyper parameter tuning\n",
    "\n",
    "### **3.1 Manual hyper parameter tuning**\n",
    "Spend some time (20 to 30 minutes) tuning the network architecture (number of layers, number of nodes per layer, activation function) and other hyper parameters (optimizer, learning rate, batch size, number of epochs, degree of regularization). For example, try a much deeper network. How much does the training time increase for a network with 10 layers?\n",
    "\n",
    "#### **<span style=\"color:red\">Question</span>**\n",
    "14. How high classification accuracy can you achieve for the test data? What is your best configuration?\n",
    "   \n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Build and train model\n",
    "best_model = ???\n",
    "\n",
    "best_history = ???\n",
    "# Evaluate model on test data\n",
    "best_score = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print('Test loss: %.4f' % best_score[0])\n",
    "print('Test accuracy: %.4f' % best_score[1])\n",
    "\n",
    "# Plot the history from the training run\n",
    "plot_results(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Automatic hyper parameter search**\n",
    "The number of hyper parameters that can be tried manually is limited and the process of trying out the different combinations and keeping track of them is time consuming and tedious. Today, there are several libraries available for automatic hyper parameter tuning (see an extensive list [here](https://github.com/balavenkatesh3322/hyperparameter_tuning)). The library that we will use in this lab is `Ray Tune` which can be integrated with many of the deep learning APIs available today (for the full description of the library capabilities see the [documentation](https://docs.ray.io/en/latest/tune/index.html)).\n",
    "\n",
    "\n",
    "To use the `Ray Tune` functionality we need a function that defines the model training. This will then be used in a wrapper function that defines the hyper parameter search space, the resources available for running the search and the search algorithm. \n",
    "\n",
    "Start by implementing the `train_DNN` function in the `utilities.py` file (more detailed instructions are available in `utilities.py`). In the cell below, you can set up the search space and a `tune` ray object that takes the `train_DNN`. The tuner will set `train_DNN`, will select a set of hyper parameters and train several models for us (more information [here](https://docs.ray.io/en/latest/tune/key-concepts.html?_gl=1*j3ryje*_up*MQ..*_ga*NzQyMjIzNzg4LjE3MzY0MTk5MzY.*_ga_0LCWHW1N3S*MTczNjQxOTkzNS4xLjAuMTczNjQxOTkzNS4wLjAuMA..#tune-60-seconds)). \n",
    "\n",
    "\n",
    "`Ray Tune` library provides several types of hyper parameter search algorithms, including random and grid search, and Bayesian optimization. In this lab we will be using the default Ray Tune opitmization algorithm which is random serach. More infromation about the available search arlgorithms can be found [here](https://docs.ray.io/en/latest/tune/api/suggestion.html).\n",
    "\n",
    "#### **<span style=\"color:red\">Question</span>**\n",
    "15. Run the automatic hyper parameter search with range of possible hyper parameter values as in your manual search. Does the automatic search set of parameters match those that you have found?\n",
    "16. What are the benefits and drawbacks of automatic hyper parameter search?\n",
    "   \n",
    "#### **<span style=\"color:green\">Answers</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_DNN\n",
    "from utilities import train_DNN\n",
    "\n",
    "# imports for hyperparameter tuning\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "# --------------------------------------------  \n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Define the hyper parameter, both those that should be searched and those that are fixed.\n",
    "#  Hyperparameters to search are: act_fun, optimizer, use_bn, n_hidden_layers and n_hidden_units.\n",
    "# The remaining parameters can be set to fixed values (This is to reduce the search space and time).\n",
    "# Add the batch size and epochs so that the train_DNN can access them.\n",
    "hyperparameter_space =  {\n",
    "            \"act_fun\": tune.choice(???),\n",
    "            \"optimizer\": tune.choice(???),\n",
    "            \"use_bn\": tune.choice(???),\n",
    "            \"n_hidden_layers\": ???,\n",
    "            \"n_hidden_units\": ???,\n",
    "            # here define the fixed parameters\n",
    "            \"loss\": ???, \n",
    "            \"learning_rate\": ???,\n",
    "            \"use_dropout\": ???,\n",
    "            \"use_custom_dropout\": ???,\n",
    "            \"use_variational_layer\": ???,\n",
    "            \"input_shape\": ???,\n",
    "        }\n",
    "\n",
    "# specify batch and number of epochs\n",
    "training_config = {\n",
    "            \"data\": ???,\n",
    "            \"epochs\": ???,\n",
    "            \"batch_size\": ???\n",
    "        }\n",
    "\n",
    "# specify the number of samples to take from the hyper parameter space and run. The larger the number, the longer the search time.\n",
    "# Start small (e.g. 2) to test your implementation, then increase.\n",
    "num_samples = ???\n",
    "\n",
    "# ============================================\n",
    "\n",
    "\n",
    "# Definition of the Scheduler. This allows for several models to be trained/stopped/re-started simultaneously \n",
    "sched = AsyncHyperBandScheduler(\n",
    "    metric=\"mean_accuracy\", mode=\"max\",\n",
    "        time_attr=\"training_iteration\", max_t=400, grace_period=20\n",
    "    )\n",
    "\n",
    "# Setting up the tuner.\n",
    "tuner = tune.Tuner(\n",
    "        tune.with_resources(tune.with_parameters(train_DNN, training_config=training_config), resources={\"cpu\": 12, \"gpu\":1}), # definition of which training function to use and the available resources. Consider adding \"gpu\":0 to resources if available.\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=sched,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"DNN_hp_tuning\",\n",
    "            stop={\"mean_accuracy\": 1},\n",
    "            storage_path='/home/iulta54/Desktop/course_732A83/Student_version/2_DNN' # where to save the summary of the hyper parameter tuning.\n",
    "        ),\n",
    "        param_space=hyperparameter_space,\n",
    "    )\n",
    "\n",
    "# Run the hyper parameter search.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best hyper parameter configuration that was found\n",
    "best_trial = results.get_best_result(metric=\"mean_accuracy\", mode='max')\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.metrics['keras_info']['val_loss']:0.4f}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.metrics['keras_info']['val_accuracy']:0.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Uncertainty quantification\n",
    "\n",
    "In the next sections you will explore three methods for model uncertainty estimation:\n",
    "- Monte Carlo dropout where we take advantage of the dropout layer during inference time.\n",
    "- Cross validation where we train several models on different splits of data.\n",
    "- Bayesian neural networks (BNN) where we modify our model definition to allow the model to learn distributions over weights and the output. \n",
    "\n",
    "**!Note**: through the next sections, use your best model configuration that you found through hyper parameter tuning (either manual or automatic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Dropout uncertainty**\n",
    "\n",
    "Dropout can also be used during testing, to obtain an estimate of the model uncertainty. Since dropout will randomly remove connections, the network will produce different results every time the same (test) data is put into the network. This technique is called Monte Carlo dropout. For more information, see this [paper](http://proceedings.mlr.press/v48/gal16.pdf)\n",
    "\n",
    "To achieve this, we need to redefine the Keras Dropout. This was already done for you and it is available in `utilities.py` under `myDropout`. Adapt the `build_DNN` function to two boolean arguments, use_dropout and use_custom_dropout; add a standard Dropout layer if use_dropout is true, add a `myDropout` layer if use_custom_dropout is true.\n",
    "\n",
    "Run the same test data through the trained network 100 times, with dropout turned on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Your best training parameters\n",
    "batch_size = ???\n",
    "epochs = ???\n",
    "input_shape = ???\n",
    "loss = ???\n",
    "learning_rate = ???\n",
    "\n",
    "# Build and train model\n",
    "model10 = ???\n",
    "\n",
    "history10 = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times to evalute the model on test data, \n",
    "# if you get slightly different test accuracy every time, Dropout during testing is working\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model10.evaluate(Xtest, Ytest, verbose=0)\n",
    "                       \n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# === Your code here =========================\n",
    "# ============================================\n",
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "???\n",
    "\n",
    "# Calculate and print mean and std of accuracies\n",
    "???\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2: Cross validation uncertainty**\n",
    "\n",
    "Cross validation (CV) is often used to evaluate a model, by training and testing using different subsets of the data it is possible to get the uncertainty as the standard deviation over folds. We here use a [help function from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to setup the CV. Use 10 folds with shuffling, random state 1234. \n",
    "\n",
    "Note: We here assume that you have found the best hyper parameters, so here the data are only split into training and testing, no validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "random_state = 1234 \n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "n_splits = ???\n",
    "skf = StratifiedKFold(???)\n",
    "\n",
    "# Define where to save the test accuracies\n",
    "test_accuracies = ???\n",
    "\n",
    "# Loop over cross validation folds\n",
    "for ??? in ???:\n",
    "    ???\n",
    "    \n",
    "    # Calculate class weights for current split (remember to call the function using the input variable names e.g. class_weight='balanced', etc.)\n",
    "    class_weights = ???\n",
    "    \n",
    "    # Rebuild the DNN model, to not continue training on the previously trained model\n",
    "    model = ???)\n",
    "\n",
    "    # Fit the model with training set and class weights for this fold\n",
    "    history = ???\n",
    "    \n",
    "    # Evaluate the model using the test set for this fold\n",
    "    score =???\n",
    "    \n",
    "    # Save the test accuracy in an array\n",
    "    ???\n",
    "\n",
    "# Calculate and print mean and std of accuracies\n",
    "???\n",
    "\n",
    "# ============================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Bayesian neural networks (BNN)**\n",
    "\n",
    "Instead of mimicking Bayesian inference through MC dropout (see more details in this [paper](https://proceedings.mlr.press/v48/gal16.html)), what if we could build a model that can inherently give us a way to probe both model uncertainty (i.e. [epistemic uncertainty](https://link.springer.com/article/10.1007/s10994-021-05946-3) cause by few training samples) and data uncertainty (i.e. [aleatoric uncertainty](https://link.springer.com/article/10.1007/s10994-021-05946-3))? \n",
    "\n",
    "One way to achieve this is to train models to learn distributions over weights in the layers and over the output. This can be achieved by using TenssorFlow probability (see the [documentation](https://www.tensorflow.org/probability) for an in-depth description of all the functionalities). In this example we will only scratch the surface of the capabilities that a probabilistic deep learning approach has to offer, by addressing only model uncertainty. \n",
    "\n",
    "To start with, think about a BNN as an extension of your classical DNN, where during training instead of learning a weights for each 'connection' in the network, we ask the model to lean a distribution. After model training, during inference time, we sample from the learned distributions to obtain the weights used in the model in the forward pass. The modifications that we have to implement in our `build_DNN` function are the following:\n",
    "- Substitute the hidden `Dense` layers with [`DenseVariational`](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational) layers (this can be found under tensorflow-probability.layers). Do not forget to specify the `kl_weight` as 1/ number of iterations per epoch ([reference](https://arxiv.org/abs/1505.05424)). \n",
    "- Define the prior weights distribution (`make_prior_fn` input in the `DenseVariational`): this is the distribution that we expect the weights to have prior having seen the data. In this example we will not train the prior distribution.\n",
    "- Define the posterior weights distribution (`make_posterior_fn` input in the `DenseVariational`): this is the distribution that we expect the model to learn during training. Since we do not know at priory which is the posterior distribution, we let it be very general (i.e. multivariate Gaussian distribution) which parameters are learned by the model. \n",
    "\n",
    "You will find the `BNN_prior` and `BNN_posterior` defined for you in the `utilities.py` file. Update the `build_DNN` to accept a new boolean input `use_variational_layer` that when true substitutes the hidden `Dense` layers with the `DenseVariational` layers. \n",
    "\n",
    "**!NOTE** As you will see, training the BNN model is more computationally demanding and is more prone to over-fitting. Reduce the learning rate and increase the number of epochs to address this issue.\n",
    "\n",
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "17. (MC dropout) What is the mean and the standard deviation of the test accuracy after evaluating the model on 100 times?\n",
    "18. (CV) What is the mean and the standard deviation of the test accuracy?\n",
    "19. (CV) What is the main advantage of dropout compared to CV for estimating test uncertainty? The difference may not be so large in this notebook, but imagine that you have a network that takes 24 hours to train.\n",
    "20. (BNN) Build the BNN model and look at the number of parameters. Is there a difference between the previous DNN and the BNN? Why is that?\n",
    "21. (BNN) Without training the model, evaluate the model a twice on the validation set: do you obtain the same validation accuracy? Why\n",
    "22. Think of at least one advantage and one disadvantage for each of the three uncertainty estimation methods.\n",
    "    \n",
    "    \n",
    "#### **<span style=\"color:green\">Answer</span>**\n",
    "[Your answers here]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "# Your best training parameters\n",
    "batch_size = ???\n",
    "epochs = ???\n",
    "input_shape = ???\n",
    "loss = ???\n",
    "learning_rate = ???\n",
    "\n",
    "# Build and train model\n",
    "model11 = ???\n",
    "\n",
    "history11 = ???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done for the MC dropout uncertainty estimation, run the evaluation on the test set 100 times and show the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# === Your code here =========================\n",
    "# ============================================\n",
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "\n",
    "n_runs = ???\n",
    "\n",
    "# Define where to save the test accuracies\n",
    "test_accuracies = ???\n",
    "\n",
    "for _ in ???:\n",
    "    # predict the test set\n",
    "    pred = ???\n",
    "\n",
    "    # Save predictions\n",
    "    ???\n",
    "    \n",
    "# Calculate and print mean and std of accuracies\n",
    "???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: DNN for regression\n",
    "\n",
    "A similar DNN can be used for regression, instead of classification.\n",
    "\n",
    "#### **<span style=\"color:red\">Questions</span>**\n",
    "23. How would you change the DNN used in this lab in order to use it for regression instead?\n",
    "\n",
    "#### **<span style=\"color:green\">Answer</span>**\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Send in this jupyter notebook, with answers to all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "732a83_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
